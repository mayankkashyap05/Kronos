data:
  # Path to your 1h data (assuming standard naming convention)
  data_path: "./data/solusdt_data/1h.csv"
  
  # 1h Data Configuration
  # Lookback: 336 hours = 14 Days (2 full weeks of context)
  # Predict: 24 hours = 1 Full Day forecast
  lookback_window: 336
  predict_window: 24
  max_context: 512
  
  # 1H candles have slightly lower volatility than 15m outliers, 
  # but we keep clip high for crypto.
  clip: 10.0 
  
  # Standard split for limited data
  train_ratio: 0.9
  val_ratio: 0.1
  test_ratio: 0.0

training:
  # --- CRITICAL ADJUSTMENT FOR SMALL DATASET ---
  # Dataset is only ~8,760 rows. 
  # A Batch Size of 256 would result in only ~30 steps per epoch (too few).
  # We reduce Batch Size to 64 to get ~120 steps per epoch for better convergence.
  batch_size: 64
  accumulation_steps: 1
  num_workers: 4  # Reduced for smaller IO load

  # Epochs: Sufficient to converge on small data without memorizing noise
  tokenizer_epochs: 20
  basemodel_epochs: 40
  
  seed: 42
  
  # Learning Rates: Standard fine-tuning range
  tokenizer_learning_rate: 0.0001 
  predictor_learning_rate: 0.00002 
  
  adam_beta1: 0.9
  adam_beta2: 0.98
  adam_weight_decay: 0.05 # Reduced weight decay for smaller dataset

model_paths:
  # Use Kronos-Small if you have it, otherwise Mini is acceptable for this data size
  pretrained_tokenizer: "../models/kronos_tokenizer_2k"
  pretrained_predictor: "../models/kronos_mini" 
  
  exp_name: "SOLUSDT_1H_1Y_TEST"
  base_path: "./finetuned/"
  
  # Auto-generated paths (leave empty)
  base_save_path: "" 
  finetuned_tokenizer: ""
  tokenizer_save_name: "tokenizer"
  basemodel_save_name: "basemodel"

experiment:
  name: "SOLUSDT_1H_TEST"
  description: "Test run on 1 year of 1h data (14-day lookback)"
  use_comet: false
  train_tokenizer: true
  train_basemodel: true
  skip_existing: false

device:
  use_cuda: true
  device_id: 0
  use_ddp: false